{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dd3ebce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-12 23:04:59.376143: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow_federated import python as tff\n",
    "from collections import OrderedDict\n",
    "from tensorflow import keras\n",
    "import linecache\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import time\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25677175",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 4 \n",
    "BATCH_SIZE = 40\n",
    "SHUFFLE_BUFFER = 100\n",
    "PREFETCH_BUFFER = 10\n",
    "time_steps = 48\n",
    "interval = 1000 \n",
    "future_steps = 12\n",
    "split = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12a8a015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dataset):\n",
    "    def batch_format_fn(x_d, y_d):\n",
    "        return OrderedDict(x=x_d, y=y_d)\n",
    "\n",
    "    return dataset.repeat(NUM_EPOCHS).shuffle(SHUFFLE_BUFFER, seed=1).batch(\n",
    "      BATCH_SIZE).map(batch_format_fn).prefetch(PREFETCH_BUFFER)\n",
    "    \n",
    "def create_dataset_fed(files, lower, upper):\n",
    "    xs, ys = [], []\n",
    "    print(\"\\n#######################START\", lower, upper)\n",
    "    for file in files:\n",
    "        x_t, y_t = [], []\n",
    "        data = file[lower:upper]\n",
    "        if data:\n",
    "            if (len(data) - time_steps - 1 - future_steps) < 0:\n",
    "                print(\"EMPTY\", (len(data) - time_steps - 1 - future_steps))\n",
    "            for i in range(len(data) - time_steps - 1 - future_steps):\n",
    "                v = data[i:(i + time_steps)] \n",
    "                z = data[(i + time_steps):(i + time_steps + future_steps)]\n",
    "                if check_nulls(v) and check_nulls(z):\n",
    "                    x_t.append(v)\n",
    "                    y_t.append(z)\n",
    "                else:\n",
    "                    print(\"null-\", i, end=\" \")\n",
    "            x_t = np.array(x_t)[:,:,np.newaxis]\n",
    "            y_t = np.array(y_t)[:,:,np.newaxis]\n",
    "            xs.append(x_t)\n",
    "            ys.append(y_t)\n",
    "        else:\n",
    "            print(\"no data\" )\n",
    "    xs = np.array(xs)\n",
    "    ys = np.array(ys)\n",
    "    # return [tf.data.Dataset.from_tensor_slices((Xs[x],  np.array(ys[x]))) for x in range(len(Xs))]\n",
    "    # return [ tf.data.Dataset.from_tensor_slices((Xs[x],  np.array(ys[x]))) for x in range(len(Xs))] - removed brackets, and np.array\n",
    "    tikva = [tf.data.Dataset.from_tensor_slices((xs[x], ys[x])) for x in range(len(xs))]\n",
    "    print(\"\\n#######################END\", lower, upper)\n",
    "    return tikva\n",
    "\n",
    "def make_federated_data(files, lower, upper):\n",
    "    data = create_dataset_fed(files, lower, upper)\n",
    "    return [preprocess(x) for x in data if x]\n",
    "\n",
    "def create_keras_model():\n",
    "    return tf.keras.models.Sequential([\n",
    "      keras.layers.LSTM(64, input_shape=(time_steps, 1)),\n",
    "      keras.layers.Dense(12),\n",
    "    ])\n",
    "\n",
    "def model_fn():\n",
    "    # We _must_ create a new model here, and _not_ capture it from an external\n",
    "    # scope. TFF will call this within different graph contexts.\n",
    "    keras_model = create_keras_model()\n",
    "    return tff.learning.from_keras_model(\n",
    "      keras_model,\n",
    "      input_spec=preprocessed_example_dataset.element_spec,\n",
    "      loss=tf.keras.losses.MeanSquaredError(),\n",
    "      metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "def check_nulls(data):\n",
    "    if not(all(is_float(ele) for ele in data)):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "def is_float(element):\n",
    "    try:\n",
    "        float(element)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9561aafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clusters = [['3081.txt', '1318.txt', '2202.txt', '2574.txt', '4111.txt', '1727.txt', '1055.txt', '3871.txt', '2776.txt', '3050.txt', '3133.txt', '3046.txt', '3195.txt', '3585.txt', '2922.txt', '3427.txt', '1143.txt', '1343.txt', '2474.txt', '1627.txt', '2065.txt', '2529.txt', '2647.txt', '3405.txt', '3820.txt', '3599.txt', '2536.txt', '3167.txt', '1980.txt', '3617.txt', '2680.txt', '3777.txt', '1809.txt', '1660.txt', '1059.txt', '2791.txt', '2828.txt', '2964.txt', '1840.txt', '1711.txt', '1117.txt', '3073.txt'], \n",
    "            ['3041.txt', '1447.txt', '3330.txt', '4163.txt', '4121.txt', '2838.txt'], \n",
    "            ['4076.txt', '2407.txt', '3296.txt', '3447.txt', '4049.txt', '1904.txt', '1086.txt', '4129.txt', '3497.txt', '3910.txt', '2239.txt', '1081.txt', '1839.txt', '3036.txt', '2315.txt', '2301.txt', '2067.txt', '1180.txt', '1619.txt', '2304.txt', '3346.txt', '3781.txt', '2893.txt', '2501.txt', '1950.txt', '2685.txt', '2121.txt', '2519.txt', '3349.txt', '1827.txt', '2081.txt', '2522.txt', '1610.txt', '3359.txt', '1404.txt', '1834.txt', '2235.txt', '1113.txt', '3843.txt', '2593.txt', '1515.txt', '2424.txt', '4055.txt', '1312.txt', '3931.txt', '1115.txt', '3884.txt', '1063.txt', '2595.txt', '2436.txt', '2153.txt'], \n",
    "            ['1330.txt']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cb4d7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'3081.txt': 0, '1318.txt': 0, '2202.txt': 0, '2574.txt': 0, '4111.txt': 0, '1727.txt': 0, '1055.txt': 0, '3871.txt': 0, '2776.txt': 0, '3050.txt': 0, '3133.txt': 0, '3046.txt': 0, '3195.txt': 0, '3585.txt': 0, '2922.txt': 0, '3427.txt': 0, '1143.txt': 0, '1343.txt': 0, '2474.txt': 0, '1627.txt': 0, '2065.txt': 0, '2529.txt': 0, '2647.txt': 0, '3405.txt': 0, '3820.txt': 0, '3599.txt': 0, '2536.txt': 0, '3167.txt': 0, '1980.txt': 0, '3617.txt': 0, '2680.txt': 0, '3777.txt': 0, '1809.txt': 0, '1660.txt': 0, '1059.txt': 0, '2791.txt': 0, '2828.txt': 0, '2964.txt': 0, '1840.txt': 0, '1711.txt': 0, '1117.txt': 0, '3073.txt': 0, '3041.txt': 1, '1447.txt': 1, '3330.txt': 1, '4163.txt': 1, '4121.txt': 1, '2838.txt': 1, '4076.txt': 2, '2407.txt': 2, '3296.txt': 2, '3447.txt': 2, '4049.txt': 2, '1904.txt': 2, '1086.txt': 2, '4129.txt': 2, '3497.txt': 2, '3910.txt': 2, '2239.txt': 2, '1081.txt': 2, '1839.txt': 2, '3036.txt': 2, '2315.txt': 2, '2301.txt': 2, '2067.txt': 2, '1180.txt': 2, '1619.txt': 2, '2304.txt': 2, '3346.txt': 2, '3781.txt': 2, '2893.txt': 2, '2501.txt': 2, '1950.txt': 2, '2685.txt': 2, '2121.txt': 2, '2519.txt': 2, '3349.txt': 2, '1827.txt': 2, '2081.txt': 2, '2522.txt': 2, '1610.txt': 2, '3359.txt': 2, '1404.txt': 2, '1834.txt': 2, '2235.txt': 2, '1113.txt': 2, '3843.txt': 2, '2593.txt': 2, '1515.txt': 2, '2424.txt': 2, '4055.txt': 2, '1312.txt': 2, '3931.txt': 2, '1115.txt': 2, '3884.txt': 2, '1063.txt': 2, '2595.txt': 2, '2436.txt': 2, '2153.txt': 2, '1330.txt': 3}\n",
      "3081.txt 2\n",
      "[12337, 12338]\n",
      "1318.txt 2\n",
      "[12337, 12338]\n",
      "2202.txt 2\n",
      "[12337, 12338]\n",
      "2574.txt 2\n",
      "[12337, 12338]\n",
      "4111.txt 2\n",
      "[12337, 12338]\n",
      "1727.txt 2\n",
      "[12337, 12338]\n",
      "1055.txt 2\n",
      "[12337, 12338]\n",
      "3871.txt 2\n",
      "[12337, 12338]\n",
      "2776.txt 2\n",
      "[12337, 12338]\n",
      "3050.txt 2\n",
      "[12337, 12338]\n",
      "3133.txt 2\n",
      "[12337, 12338]\n",
      "3046.txt 2\n",
      "[12337, 12338]\n",
      "3195.txt 2\n",
      "[12337, 12338]\n",
      "3585.txt 2\n",
      "[12337, 12338]\n",
      "2922.txt 2\n",
      "[12337, 12338]\n",
      "3427.txt 2\n",
      "[12337, 12338]\n",
      "1143.txt 2\n",
      "[12337, 12338]\n",
      "1343.txt 2\n",
      "[12337, 12338]\n",
      "2474.txt 2\n",
      "[12337, 12338]\n",
      "1627.txt 2\n",
      "[12337, 12338]\n",
      "2065.txt 2\n",
      "[12337, 12338]\n",
      "2529.txt 2\n",
      "[12337, 12338]\n",
      "2647.txt 2\n",
      "[12337, 12338]\n",
      "3405.txt 2\n",
      "[12337, 12338]\n",
      "3820.txt 2\n",
      "[12337, 12338]\n",
      "3599.txt 2\n",
      "[12337, 12338]\n",
      "2536.txt 2\n",
      "[12337, 12338]\n",
      "3167.txt 2\n",
      "[12337, 12338]\n",
      "1980.txt 2\n",
      "[12337, 12338]\n",
      "3617.txt 2\n",
      "[12337, 12338]\n",
      "2680.txt 2\n",
      "[12337, 12338]\n",
      "3777.txt 2\n",
      "[12337, 12338]\n",
      "1809.txt 2\n",
      "[12337, 12338]\n",
      "1660.txt 2\n",
      "[12337, 12338]\n",
      "1059.txt 2\n",
      "[12337, 12338]\n",
      "2791.txt 2\n",
      "[12337, 12338]\n",
      "2828.txt 2\n",
      "[12337, 12338]\n",
      "2964.txt 2\n",
      "[12337, 12338]\n",
      "1840.txt 2\n",
      "[12337, 12338]\n",
      "1711.txt 2\n",
      "[12337, 12338]\n",
      "1117.txt 2\n",
      "[12337, 12338]\n",
      "3073.txt 2\n",
      "[12337, 12338]\n",
      "3041.txt 2\n",
      "[12337, 12338]\n",
      "1447.txt 2\n",
      "[12337, 12338]\n",
      "3330.txt 2\n",
      "[12337, 12338]\n",
      "4163.txt 2\n",
      "[12337, 12338]\n",
      "4121.txt 2\n",
      "[12337, 12338]\n",
      "2838.txt 2\n",
      "[12337, 12338]\n",
      "4076.txt 2\n",
      "[12337, 12338]\n",
      "2407.txt 2\n",
      "[12337, 12338]\n",
      "3296.txt 2\n",
      "[12337, 12338]\n",
      "3447.txt 2\n",
      "[12337, 12338]\n",
      "4049.txt 2\n",
      "[12337, 12338]\n",
      "1904.txt 2\n",
      "[12337, 12338]\n",
      "1086.txt 2\n",
      "[12337, 12338]\n",
      "4129.txt 2\n",
      "[12337, 12338]\n",
      "3497.txt 2\n",
      "[12337, 12338]\n",
      "3910.txt 2\n",
      "[12337, 12338]\n",
      "2239.txt 2\n",
      "[12337, 12338]\n",
      "1081.txt 2\n",
      "[12337, 12338]\n",
      "1839.txt 2\n",
      "[12337, 12338]\n",
      "3036.txt 2\n",
      "[12337, 12338]\n",
      "2315.txt 2\n",
      "[12337, 12338]\n",
      "2301.txt 2\n",
      "[12337, 12338]\n",
      "2067.txt 2\n",
      "[12337, 12338]\n",
      "1180.txt 2\n",
      "[12337, 12338]\n",
      "1619.txt 2\n",
      "[12337, 12338]\n",
      "2304.txt 2\n",
      "[12337, 12338]\n",
      "3346.txt 2\n",
      "[12337, 12338]\n",
      "3781.txt 2\n",
      "[12337, 12338]\n",
      "2893.txt 2\n",
      "[12337, 12338]\n",
      "2501.txt 2\n",
      "[12337, 12338]\n",
      "1950.txt 2\n",
      "[12337, 12338]\n",
      "2685.txt 2\n",
      "[12337, 12338]\n",
      "2121.txt 2\n",
      "[12337, 12338]\n",
      "2519.txt 2\n",
      "[12337, 12338]\n",
      "3349.txt 2\n",
      "[12337, 12338]\n",
      "1827.txt 2\n",
      "[12337, 12338]\n",
      "2081.txt 2\n",
      "[12337, 12338]\n",
      "2522.txt 2\n",
      "[12337, 12338]\n",
      "1610.txt 2\n",
      "[12337, 12338]\n",
      "3359.txt 2\n",
      "[12337, 12338]\n",
      "1404.txt 2\n",
      "[12337, 12338]\n",
      "1834.txt 2\n",
      "[12337, 12338]\n",
      "2235.txt 2\n",
      "[12337, 12338]\n",
      "1113.txt 2\n",
      "[12337, 12338]\n",
      "3843.txt 2\n",
      "[12337, 12338]\n",
      "2593.txt 2\n",
      "[12337, 12338]\n",
      "1515.txt 2\n",
      "[12337, 12338]\n",
      "2424.txt 2\n",
      "[12337, 12338]\n",
      "4055.txt 2\n",
      "[12337, 12338]\n",
      "1312.txt 2\n",
      "[12337, 12338]\n",
      "3931.txt 2\n",
      "[12337, 12338]\n",
      "1115.txt 2\n",
      "[12337, 12338]\n",
      "3884.txt 2\n",
      "[12337, 12338]\n",
      "1063.txt 2\n",
      "[12337, 12338]\n",
      "2595.txt 2\n",
      "[12337, 12338]\n",
      "2436.txt 2\n",
      "[12337, 12338]\n",
      "2153.txt 2\n",
      "[12337, 12338]\n",
      "1330.txt 2\n",
      "[12337, 12338]\n"
     ]
    }
   ],
   "source": [
    "# Read files in a dictionary with key = file name, and value = file\n",
    "\n",
    "C1086 = len(clusters)\n",
    "PATH = './ExperementData/'\n",
    "\n",
    "alloc = {}\n",
    "for c in range(len(clusters)):\n",
    "    for name in clusters[c]:\n",
    "        alloc[name] = c\n",
    "print(alloc)\n",
    "        \n",
    "files_in_cluster = []\n",
    "for c in range(len(clusters) + 1):\n",
    "    files_in_cluster.append([])\n",
    "\n",
    "data = []\n",
    "\n",
    "randomDict = {}\n",
    "\n",
    "for cl in clusters:\n",
    "    for f in cl:\n",
    "        with open(PATH + f,'r') as reader:\n",
    "            temp = []\n",
    "            randomDict[f] = []\n",
    "            for l, line in enumerate(reader):\n",
    "                if line.strip() == 'Null':\n",
    "                    temp.append('Null')\n",
    "                    randomDict[f].append(l)\n",
    "                else:\n",
    "                    temp.append(float(line.strip()))\n",
    "\n",
    "            print(f, len(randomDict[f]))\n",
    "            if len(randomDict[f]) < 51:\n",
    "                print(randomDict[f])\n",
    "            data.append(temp)\n",
    "            files_in_cluster[alloc[f]].append(temp)\n",
    "            if f == \"1086.txt\":\n",
    "                files_in_cluster[C1086].append(temp)\n",
    "\n",
    "test = [d[int(25727*split):] for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7eb59d22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#######################START 1 500\n",
      "\n",
      "#######################END 1 500\n",
      "10036\n",
      "\n",
      "#######################START 10036 11036\n",
      "\n",
      "#######################END 10036 11036\n",
      "round  0, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('root_mean_squared_error', 0.86599404), ('loss', 0.74994564), ('num_examples', 157752), ('num_batches', 3948)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
      "1118\n",
      "\n",
      "#######################START 1118 2118\n",
      "\n",
      "#######################END 1118 2118\n",
      "round  1, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('root_mean_squared_error', 2.8518505), ('loss', 8.13305), ('num_examples', 22536), ('num_batches', 564)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
      "8775\n",
      "\n",
      "#######################START 8775 9775\n",
      "\n",
      "#######################END 8775 9775\n",
      "round  2, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('root_mean_squared_error', 0.51514834), ('loss', 0.26537785), ('num_examples', 191556), ('num_batches', 4794)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
      "11256\n",
      "\n",
      "#######################START 11256 12256\n",
      "\n",
      "#######################END 11256 12256\n",
      "round  3, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('root_mean_squared_error', 8.880676), ('loss', 78.866425), ('num_examples', 3756), ('num_batches', 94)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n"
     ]
    }
   ],
   "source": [
    "example_dataset = create_dataset_fed(files_in_cluster[C1086], 1, 500)[0]\n",
    "preprocessed_example_dataset = preprocess(example_dataset)\n",
    "process = [tff.learning.algorithms.build_weighted_fed_avg(\n",
    "    model_fn,\n",
    "    client_optimizer_fn=lambda: keras.optimizers.Adam(0.001),\n",
    "    server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1.0)) for _ in range(len(clusters))]\n",
    "          \n",
    "state = [process[x].initialize() for x in range (len(process))]\n",
    "for c in range(len(clusters)):\n",
    "#     max_time = (len(clusters[c])/60)*9000\n",
    "    max_time = (len(clusters[c])/60)*9\n",
    "    start_time = time.time()\n",
    "    while time.time() - start_time < max_time:\n",
    "        location = random.randint(1, int(25727*split)-interval)\n",
    "        print(location)\n",
    "        federated_train_data = make_federated_data(files_in_cluster[c], location, location + interval)\n",
    "        if len(federated_train_data) > 0:\n",
    "            result = process[c].next(state[c], federated_train_data)\n",
    "            state[c] = result.state\n",
    "            metrics = result.metrics\n",
    "            print('round {:2d}, metrics={}'.format(c, metrics))\n",
    "        else:\n",
    "            print(\"empty main\", location)\n",
    "#     tensorflow 2.9.0 and federated 0.40.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ac4e51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data, time_steps=1):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(data) - time_steps-12):\n",
    "        v = data[i:(i + time_steps)]\n",
    "        z = data[(i + time_steps):(i + time_steps+12)]\n",
    "        if check_nulls(z) and check_nulls(v):\n",
    "            ys.append(z)\n",
    "            Xs.append(v)\n",
    "    return np.array(Xs), np.array(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47706720",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159/159 [==============================] - 3s 9ms/step\n",
      "159/159 [==============================] - 3s 10ms/step\n",
      "159/159 [==============================] - 3s 9ms/step\n",
      "159/159 [==============================] - 3s 11ms/step\n",
      "159/159 [==============================] - 3s 10ms/step\n",
      "159/159 [==============================] - 3s 9ms/step\n",
      "159/159 [==============================] - 3s 9ms/step\n",
      "159/159 [==============================] - 3s 10ms/step\n",
      "159/159 [==============================] - 3s 10ms/step\n",
      "159/159 [==============================] - 3s 9ms/step\n",
      "159/159 [==============================] - 3s 12ms/step\n",
      "159/159 [==============================] - 3s 10ms/step\n",
      "159/159 [==============================] - 3s 10ms/step\n",
      "159/159 [==============================] - 3s 10ms/step\n",
      "159/159 [==============================] - 3s 9ms/step\n",
      "159/159 [==============================] - 3s 11ms/step\n",
      "159/159 [==============================] - 3s 10ms/step\n",
      "159/159 [==============================] - 3s 10ms/step\n",
      "159/159 [==============================] - 3s 10ms/step\n",
      "159/159 [==============================] - 3s 10ms/step\n",
      "159/159 [==============================] - 3s 9ms/step\n",
      "159/159 [==============================] - 3s 10ms/step\n",
      "159/159 [==============================] - 3s 9ms/step\n",
      "159/159 [==============================] - 3s 10ms/step\n",
      "159/159 [==============================] - 3s 9ms/step\n",
      "159/159 [==============================] - 3s 9ms/step\n",
      "159/159 [==============================] - 3s 9ms/step\n",
      "159/159 [==============================] - 3s 10ms/step\n",
      "159/159 [==============================] - 3s 9ms/step\n",
      "159/159 [==============================] - 3s 9ms/step\n",
      "159/159 [==============================] - 3s 9ms/step\n",
      "159/159 [==============================] - 3s 9ms/step\n",
      "159/159 [==============================] - 3s 10ms/step\n",
      "159/159 [==============================] - 3s 9ms/step\n",
      "159/159 [==============================] - 3s 9ms/step\n",
      "159/159 [==============================] - 3s 10ms/step\n",
      "159/159 [==============================] - 3s 9ms/step\n",
      "159/159 [==============================] - 3s 10ms/step\n",
      "159/159 [==============================] - 2s 9ms/step\n",
      "159/159 [==============================] - 3s 9ms/step\n",
      "159/159 [==============================] - 3s 10ms/step\n",
      "159/159 [==============================] - 3s 9ms/step\n",
      "159/159 [==============================] - 3s 10ms/step\n",
      "159/159 [==============================] - 3s 11ms/step\n",
      "159/159 [==============================] - 3s 9ms/step\n",
      "159/159 [==============================] - 3s 10ms/step\n",
      "159/159 [==============================] - 3s 10ms/step\n",
      "159/159 [==============================] - 3s 9ms/step\n",
      "159/159 [==============================] - 3s 9ms/step\n",
      "159/159 [==============================] - 3s 9ms/step\n",
      "159/159 [==============================] - 3s 9ms/step\n",
      "159/159 [==============================] - 3s 9ms/step\n",
      "159/159 [==============================] - 2s 9ms/step\n",
      "159/159 [==============================] - 3s 10ms/step\n",
      "159/159 [==============================] - 2s 9ms/step\n",
      "159/159 [==============================] - 3s 10ms/step\n",
      "159/159 [==============================] - 3s 10ms/step\n",
      "159/159 [==============================] - 2s 9ms/step\n",
      "159/159 [==============================] - 3s 9ms/step\n",
      "159/159 [==============================] - 3s 10ms/step\n",
      "159/159 [==============================] - 3s 9ms/step\n",
      "159/159 [==============================] - 3s 9ms/step\n",
      "159/159 [==============================] - 3s 9ms/step\n",
      "159/159 [==============================] - 3s 9ms/step\n",
      "159/159 [==============================] - 3s 9ms/step\n",
      "159/159 [==============================] - 3s 10ms/step\n",
      "159/159 [==============================] - 3s 9ms/step\n",
      "159/159 [==============================] - 3s 9ms/step\n",
      "159/159 [==============================] - 2s 9ms/step\n",
      "159/159 [==============================] - 3s 10ms/step\n",
      "159/159 [==============================] - 2s 9ms/step\n",
      "159/159 [==============================] - 3s 10ms/step\n",
      "159/159 [==============================] - 3s 9ms/step\n",
      "159/159 [==============================] - 3s 9ms/step\n",
      "159/159 [==============================] - 3s 9ms/step\n",
      "159/159 [==============================] - 3s 9ms/step\n",
      "159/159 [==============================] - 3s 10ms/step\n",
      "159/159 [==============================] - 3s 9ms/step\n",
      "159/159 [==============================] - 2s 10ms/step\n",
      "159/159 [==============================] - 3s 9ms/step\n",
      "159/159 [==============================] - 3s 9ms/step\n",
      "159/159 [==============================] - 3s 9ms/step\n",
      "159/159 [==============================] - 3s 9ms/step\n",
      "159/159 [==============================] - 3s 10ms/step\n",
      "159/159 [==============================] - 3s 9ms/step\n",
      "159/159 [==============================] - 3s 10ms/step\n",
      "159/159 [==============================] - 3s 10ms/step\n",
      "159/159 [==============================] - 3s 10ms/step\n",
      "159/159 [==============================] - 3s 9ms/step\n",
      "159/159 [==============================] - 3s 10ms/step\n",
      "159/159 [==============================] - 3s 9ms/step\n",
      "159/159 [==============================] - 3s 9ms/step\n",
      "159/159 [==============================] - 3s 9ms/step\n",
      "159/159 [==============================] - 2s 9ms/step\n",
      "159/159 [==============================] - 3s 10ms/step\n",
      "159/159 [==============================] - 3s 10ms/step\n",
      "159/159 [==============================] - 2s 9ms/step\n",
      "159/159 [==============================] - 3s 9ms/step\n",
      "159/159 [==============================] - 3s 10ms/step\n",
      "159/159 [==============================] - 3s 9ms/step\n"
     ]
    }
   ],
   "source": [
    "onlyfiles = [f for f in listdir(PATH) if isfile(join(PATH, f))and f[-4:]==\".txt\"]\n",
    "# for i in range(len(onlyfiles)):\n",
    "#     if onlyfiles[i] == \"1086.txt\":\n",
    "#         one_file = onlyfiles[i]\n",
    "# print(one_file)\n",
    "\n",
    "for i in range(len(test)):\n",
    "    X_test, y_test = create_dataset(test[i], time_steps)\n",
    "    if X_test.size > 0 and y_test.size > 0:\n",
    "        X_test = X_test[:,:,np.newaxis]\n",
    "        y_test = y_test[:,:,np.newaxis]\n",
    "        model_for_inference = create_keras_model()\n",
    "        weights = state[alloc[onlyfiles[i]]].global_model_weights\n",
    "        weights.assign_weights_to(model_for_inference)\n",
    "        y_pred = model_for_inference.predict(X_test)\n",
    "        dataframe = pd.DataFrame(np.squeeze(np.array(y_pred)))\n",
    "        dataframe.to_csv(r\"./num_clust_four_100/pred-kmeans2/\"+onlyfiles[i][:4]+'.csv')\n",
    "        dataframe = pd.DataFrame(np.squeeze(np.array(y_test)))\n",
    "        dataframe.to_csv(r\"./num_clust_four_100/test-kmeans2/\"+onlyfiles[i][:4]+'.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e69bbfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ede153",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
