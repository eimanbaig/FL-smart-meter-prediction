{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d62b0c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-12 04:43:27.738233: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "from tensorflow import keras\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
    "import linecache\n",
    "import matplotlib.pyplot as plt\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from tensorflow_federated import python as tff\n",
    "from collections import OrderedDict\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e8c0e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 4\n",
    "BATCH_SIZE = 40\n",
    "SHUFFLE_BUFFER = 100\n",
    "PREFETCH_BUFFER = 10\n",
    "time_steps =48*3\n",
    "interval = 20000\n",
    "future_steps = 12\n",
    "split=0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72edbf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dataset):\n",
    "\n",
    "    def batch_format_fn(x_d, y_d):\n",
    "        return OrderedDict(\n",
    "            x=x_d,\n",
    "            y=y_d)\n",
    "\n",
    "    return dataset.repeat(NUM_EPOCHS).shuffle(SHUFFLE_BUFFER, seed=1).batch(\n",
    "      BATCH_SIZE).map(batch_format_fn).prefetch(PREFETCH_BUFFER)\n",
    "\n",
    "def read_subset(file,lower,upper):\n",
    "    data = []\n",
    "    for i in range(lower,upper):\n",
    "        line = linecache.getline(file, i)\n",
    "        line = line.strip('\\n')\n",
    "        data.append(line)\n",
    "    return data  \n",
    "\n",
    "def conv_float(data):\n",
    "    for i in range(len(data)):\n",
    "        data[i]=float(data[i])\n",
    "    return(data)\n",
    "    \n",
    "def is_float(element):\n",
    "    try:\n",
    "        float(element)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "def create_dataset_fed(files, lower, upper, time_steps=1):\n",
    "    Xs, ys = [], []\n",
    "    for file in files:\n",
    "        x_t, y_t=[], []\n",
    "        data = read_subset(f'./ExperementData/{file}',lower,upper)\n",
    "        if data!=False:\n",
    "            for i in range(len(data) - time_steps -1-future_steps):\n",
    "                v = data[i:(i + time_steps)] \n",
    "                z = data[(i + time_steps):(i + time_steps+future_steps)]\n",
    "                if check_nulls(z) and check_nulls(v):\n",
    "                    x_t.append(conv_float(v))\n",
    "                    y_t.append(conv_float(z))\n",
    "            x_t = np.array(x_t)[:,:,np.newaxis]\n",
    "            y_t = np.array(y_t)[:,:,np.newaxis]\n",
    "            Xs.append(x_t)\n",
    "            ys.append(y_t)\n",
    "    Xs = np.array(Xs)\n",
    "    ys = np.array(ys)\n",
    "    #return [tf.data.Dataset.from_tensor_slices((Xs[x],  np.array(ys[x]))) for x in range(len(Xs))]\n",
    "    return [ tf.data.Dataset.from_tensor_slices((Xs[x],  np.array(ys[x]))) for x in range(len(Xs))]\n",
    "\n",
    "def make_federated_data(files, lower, upper):\n",
    "    data = create_dataset_fed(files,lower, upper,time_steps)\n",
    "    return [\n",
    "      preprocess( x ) for x in data if x!=False\n",
    "    ]\n",
    "\n",
    "def create_keras_model():\n",
    "    return tf.keras.models.Sequential([\n",
    "      keras.layers.LSTM(64, input_shape=(time_steps, 1)),\n",
    "      keras.layers.Dense(12),\n",
    "    ])\n",
    "\n",
    "def model_fn():\n",
    "    # We _must_ create a new model here, and _not_ capture it from an external\n",
    "    # scope. TFF will call this within different graph contexts.\n",
    "    keras_model = create_keras_model()\n",
    "    return tff.learning.from_keras_model(\n",
    "      keras_model,\n",
    "      input_spec=preprocessed_example_dataset.element_spec,\n",
    "      loss=tf.keras.losses.MeanSquaredError(),\n",
    "      metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "def check_nulls(data):\n",
    "    if not(all(is_float(ele) for ele in data)):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "def create_dataset(data, time_steps=1):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(data) - time_steps-12):\n",
    "        v = data[i:(i + time_steps)]\n",
    "        z = data[(i + time_steps):(i + time_steps+12)]\n",
    "        if check_nulls(z) and check_nulls(v):\n",
    "            ys.append(z)\n",
    "            Xs.append(v)\n",
    "    return np.array(Xs), np.array(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8d06001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3081.txt',\n",
       " '4076.txt',\n",
       " '2407.txt',\n",
       " '3296.txt',\n",
       " '1318.txt',\n",
       " '1330.txt',\n",
       " '3447.txt',\n",
       " '3041.txt',\n",
       " '4049.txt',\n",
       " '1904.txt',\n",
       " '1086.txt',\n",
       " '2202.txt',\n",
       " '4129.txt',\n",
       " '2574.txt',\n",
       " '4111.txt',\n",
       " '1727.txt',\n",
       " '1055.txt',\n",
       " '3871.txt',\n",
       " '2776.txt',\n",
       " '3497.txt',\n",
       " '3050.txt',\n",
       " '3910.txt',\n",
       " '2239.txt',\n",
       " '3133.txt',\n",
       " '1081.txt',\n",
       " '3046.txt',\n",
       " '1447.txt',\n",
       " '3330.txt',\n",
       " '1839.txt',\n",
       " '4163.txt',\n",
       " '3195.txt',\n",
       " '3585.txt',\n",
       " '3036.txt',\n",
       " '2315.txt',\n",
       " '2301.txt',\n",
       " '2922.txt',\n",
       " '3427.txt',\n",
       " '2067.txt',\n",
       " '1180.txt',\n",
       " '1143.txt',\n",
       " '1619.txt',\n",
       " '2304.txt',\n",
       " '3346.txt',\n",
       " '1343.txt',\n",
       " '2474.txt',\n",
       " '1627.txt',\n",
       " '3781.txt',\n",
       " '2893.txt',\n",
       " '2065.txt',\n",
       " '2529.txt',\n",
       " '2501.txt',\n",
       " '1950.txt',\n",
       " '2685.txt',\n",
       " '2121.txt',\n",
       " '2647.txt',\n",
       " '3405.txt',\n",
       " '3820.txt',\n",
       " '2519.txt',\n",
       " '3349.txt',\n",
       " '1827.txt',\n",
       " '3599.txt',\n",
       " '2081.txt',\n",
       " '2536.txt',\n",
       " '2522.txt',\n",
       " '1610.txt',\n",
       " '3359.txt',\n",
       " '3167.txt',\n",
       " '1980.txt',\n",
       " '3617.txt',\n",
       " '1404.txt',\n",
       " '2680.txt',\n",
       " '3777.txt',\n",
       " '1809.txt',\n",
       " '1834.txt',\n",
       " '2235.txt',\n",
       " '1660.txt',\n",
       " '1113.txt',\n",
       " '3843.txt',\n",
       " '1059.txt',\n",
       " '4121.txt',\n",
       " '2593.txt',\n",
       " '1515.txt',\n",
       " '2791.txt',\n",
       " '2424.txt',\n",
       " '4055.txt',\n",
       " '2828.txt',\n",
       " '1312.txt',\n",
       " '2964.txt',\n",
       " '2838.txt',\n",
       " '3931.txt',\n",
       " '1840.txt',\n",
       " '1115.txt',\n",
       " '3884.txt',\n",
       " '1711.txt',\n",
       " '1063.txt',\n",
       " '2595.txt',\n",
       " '2436.txt',\n",
       " '1117.txt',\n",
       " '3073.txt',\n",
       " '2153.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "onlyfiles = [f for f in listdir('./ExperementData/') if isfile(join('./ExperementData/', f))and f[-4:]==\".txt\"]\n",
    "onlyfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a800685f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in onlyfiles:\n",
    "    with open ('./ExperementData/'+f,'r') as reader:\n",
    "        temp = []\n",
    "        for line in reader:\n",
    "            if line.strip() == 'Null':\n",
    "                temp.append('Null')\n",
    "            else:\n",
    "                temp.append(float(line.strip()))\n",
    "        data.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72599d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [np.array([d[:int(25727*split)] for d in data]).flatten()]\n",
    "test = [d[int(25727*split):] for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76b37cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2d56f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277\n",
      "0\n",
      "round  0, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('root_mean_squared_error', 0.82401735), ('loss', 0.67900485), ('num_examples', 7874400), ('num_batches', 196900)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
      "--- 12295.58718585968 seconds ---\n"
     ]
    }
   ],
   "source": [
    "NUM_ROUNDS = 5\n",
    "# is it ok that this is 95 and not 5\n",
    "\n",
    "example_dataset = create_dataset_fed(['1086.txt'],1, 500,time_steps)[0]\n",
    "preprocessed_example_dataset = preprocess(example_dataset)\n",
    "# iterative_process = tff.learning.build_federated_averaging_process(\n",
    "iterative_process = tff.learning.algorithms.build_weighted_fed_avg(\n",
    "    model_fn,\n",
    "    client_optimizer_fn=lambda: keras.optimizers.Adam(0.001),\n",
    "    server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1.0))\n",
    "state = iterative_process.initialize()\n",
    "start_time = time.time()\n",
    "for i in range(NUM_ROUNDS):\n",
    "    selected_files = onlyfiles\n",
    "    location = random.randint(1,int(25727*split)-interval)\n",
    "    print(location)\n",
    "    print(i)\n",
    "    federated_train_data = make_federated_data(selected_files, location, location+interval)\n",
    "    if len(federated_train_data)>0:\n",
    "#             state, metrics = iterative_process.next(state, federated_train_data)\n",
    "#             print('metrics={}'.format( metrics['train']))\n",
    "            result = iterative_process.next(state, federated_train_data)\n",
    "            state = result.state\n",
    "            metrics = result.metrics\n",
    "            print('round {:2d}, metrics={}'.format(i, metrics))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb90f28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_for_inference = create_keras_model()\n",
    "# state.model.assign_weights_to(model_for_inference)\n",
    "\n",
    "weights = state.global_model_weights\n",
    "weights.assign_weights_to(model_for_inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efc75d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156/156 [==============================] - 8s 31ms/step\n",
      "156/156 [==============================] - 6s 39ms/step\n",
      "156/156 [==============================] - 5s 35ms/step\n",
      "156/156 [==============================] - 4s 27ms/step\n",
      "156/156 [==============================] - 5s 32ms/step\n",
      "156/156 [==============================] - 6s 37ms/step\n",
      "156/156 [==============================] - 7s 37ms/step\n",
      "156/156 [==============================] - 5s 29ms/step\n",
      "156/156 [==============================] - 4s 28ms/step\n",
      "156/156 [==============================] - 6s 36ms/step\n",
      "156/156 [==============================] - 9s 55ms/step\n",
      "156/156 [==============================] - 8s 51ms/step\n",
      "156/156 [==============================] - 5s 31ms/step\n",
      "156/156 [==============================] - 6s 37ms/step\n",
      "156/156 [==============================] - 8s 53ms/step\n",
      "156/156 [==============================] - 8s 49ms/step\n",
      "156/156 [==============================] - 7s 48ms/step\n",
      "156/156 [==============================] - 6s 41ms/step\n",
      "156/156 [==============================] - 6s 41ms/step\n",
      "156/156 [==============================] - 5s 35ms/step\n",
      "156/156 [==============================] - 6s 39ms/step\n",
      "156/156 [==============================] - 5s 29ms/step\n",
      "156/156 [==============================] - 7s 48ms/step\n",
      "156/156 [==============================] - 7s 47ms/step\n",
      "156/156 [==============================] - 9s 55ms/step\n",
      "156/156 [==============================] - 7s 42ms/step\n",
      "156/156 [==============================] - 7s 46ms/step\n",
      "156/156 [==============================] - 10s 62ms/step\n",
      "156/156 [==============================] - 8s 48ms/step\n",
      "156/156 [==============================] - 8s 46ms/step\n",
      "156/156 [==============================] - 8s 50ms/step\n",
      "156/156 [==============================] - 9s 54ms/step\n",
      "156/156 [==============================] - 8s 47ms/step\n",
      "156/156 [==============================] - 6s 39ms/step\n",
      "156/156 [==============================] - 6s 40ms/step\n",
      "156/156 [==============================] - 5s 33ms/step\n",
      "156/156 [==============================] - 7s 46ms/step\n",
      "156/156 [==============================] - 10s 62ms/step\n",
      "156/156 [==============================] - 5s 32ms/step\n",
      "156/156 [==============================] - 5s 34ms/step\n",
      "156/156 [==============================] - 8s 49ms/step\n",
      "156/156 [==============================] - 5s 32ms/step\n",
      "156/156 [==============================] - 5s 30ms/step\n",
      "156/156 [==============================] - 6s 40ms/step\n",
      "156/156 [==============================] - 6s 41ms/step\n",
      "156/156 [==============================] - 6s 36ms/step\n",
      "156/156 [==============================] - 7s 43ms/step\n",
      "156/156 [==============================] - 6s 38ms/step\n",
      "156/156 [==============================] - 5s 32ms/step\n",
      "156/156 [==============================] - 6s 40ms/step\n",
      "156/156 [==============================] - 6s 38ms/step\n",
      "156/156 [==============================] - 8s 49ms/step\n",
      "156/156 [==============================] - 9s 57ms/step\n",
      "156/156 [==============================] - 6s 40ms/step\n",
      "156/156 [==============================] - 8s 47ms/step\n",
      "156/156 [==============================] - 7s 42ms/step\n",
      "156/156 [==============================] - 7s 47ms/step\n",
      "156/156 [==============================] - 6s 38ms/step\n",
      "156/156 [==============================] - 5s 33ms/step\n",
      "156/156 [==============================] - 8s 51ms/step\n",
      "156/156 [==============================] - 7s 44ms/step\n",
      "156/156 [==============================] - 8s 49ms/step\n",
      "156/156 [==============================] - 5s 33ms/step\n",
      "156/156 [==============================] - 7s 46ms/step\n",
      "156/156 [==============================] - 11s 72ms/step\n",
      "156/156 [==============================] - 6s 37ms/step\n",
      "156/156 [==============================] - 7s 47ms/step\n",
      "156/156 [==============================] - 5s 33ms/step\n",
      "156/156 [==============================] - 8s 50ms/step\n",
      "156/156 [==============================] - 7s 44ms/step\n",
      "156/156 [==============================] - 6s 40ms/step\n",
      "156/156 [==============================] - 6s 36ms/step\n",
      "156/156 [==============================] - 7s 47ms/step\n",
      "156/156 [==============================] - 7s 44ms/step\n",
      "156/156 [==============================] - 8s 49ms/step\n",
      "156/156 [==============================] - 6s 38ms/step\n",
      "156/156 [==============================] - 9s 55ms/step\n",
      "156/156 [==============================] - 7s 46ms/step\n",
      "156/156 [==============================] - 7s 44ms/step\n",
      "156/156 [==============================] - 6s 36ms/step\n",
      "156/156 [==============================] - 7s 44ms/step\n",
      "156/156 [==============================] - 8s 53ms/step\n",
      "156/156 [==============================] - 7s 44ms/step\n",
      "156/156 [==============================] - 7s 46ms/step\n",
      "156/156 [==============================] - 5s 31ms/step\n",
      "156/156 [==============================] - 5s 32ms/step\n",
      "156/156 [==============================] - 7s 47ms/step\n",
      "156/156 [==============================] - 6s 37ms/step\n",
      "156/156 [==============================] - 5s 34ms/step\n",
      "156/156 [==============================] - 6s 36ms/step\n",
      "156/156 [==============================] - 7s 42ms/step\n",
      "156/156 [==============================] - 9s 57ms/step\n",
      "156/156 [==============================] - 6s 39ms/step\n",
      "156/156 [==============================] - 8s 48ms/step\n",
      "156/156 [==============================] - 5s 32ms/step\n",
      "156/156 [==============================] - 9s 57ms/step\n",
      "156/156 [==============================] - 7s 42ms/step\n",
      "156/156 [==============================] - 8s 51ms/step\n",
      "156/156 [==============================] - 10s 61ms/step\n",
      "156/156 [==============================] - 9s 56ms/step\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(test)):\n",
    "    X_test, y_test = create_dataset(test[i], time_steps)\n",
    "    if X_test.size > 0 and y_test.size > 0:\n",
    "        X_test = X_test[:,:,np.newaxis]\n",
    "        y_test = y_test[:,:,np.newaxis]\n",
    "        y_pred = model_for_inference.predict(X_test)\n",
    "        dataframe = pd.DataFrame(np.squeeze(np.array(y_pred)))\n",
    "        dataframe.to_csv(r\"./no_clust_results/pred/\"+onlyfiles[i][:4]+'.csv')\n",
    "        dataframe = pd.DataFrame(np.squeeze(np.array(y_test)))\n",
    "        dataframe.to_csv(r\"./no_clust_results/test/\"+onlyfiles[i][:4]+'.csv')\n",
    "\n",
    "    \n",
    "# for i in range(len(test)):\n",
    "#     X_test, y_test = create_dataset(test[i], time_steps)\n",
    "# #     print(X_test, y_test)\n",
    "#     if X_test.size > 0 and y_test.size > 0:\n",
    "#         X_test = X_test[:,:,np.newaxis]\n",
    "#         y_test = y_test[:,:,np.newaxis]\n",
    "#         model_for_inference = create_keras_model()\n",
    "#         weights = state[alloc[onlyfiles[i]]].global_model_weights\n",
    "#     #     weights = process[c].get_model_weights(state[alloc[onlyfiles[i]]]) = guesses\n",
    "#         weights.assign_weights_to(model_for_inference)\n",
    "#     #     state[alloc[onlyfiles[i]]].model.assign_weights_to(model_for_inference) = old code\n",
    "\n",
    "#     #     process.get_model_weights(state[alloc[onlyfiles[i]]]) = what i found online\n",
    "\n",
    "#     #     assigning weights from fed model to new keras model for prediciton\n",
    "#         y_pred = model_for_inference.predict(X_test)\n",
    "#         dataframe = pd.DataFrame(np.squeeze(np.array(y_pred)))\n",
    "#     #     dataframe.to_csv(r\"./fed_weight_clust/pred/\"+onlyfiles[i][:4]+'.csv')\n",
    "#         dataframe.to_csv(r\"./num_clust_four/pred-fuzzycmeans/\"+onlyfiles[i][:4]+'.csv')\n",
    "#         dataframe = pd.DataFrame(np.squeeze(np.array(y_test)))\n",
    "#     #     dataframe.to_csv(r\"./fed_weight_clust/test/\"+onlyfiles[i][:4]+'.csv')\n",
    "#         dataframe.to_csv(r\"./num_clust_four/test-fuzzycmeans/\"+onlyfiles[i][:4]+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c751eb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_time = 2\n",
    "# start_time = time.time()\n",
    "# while time.time() -start_time < max_time:\n",
    "#     print('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd8d50a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
