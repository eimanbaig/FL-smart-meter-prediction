{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dd3ebce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-13 15:35:59.405393: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow_federated import python as tff\n",
    "from collections import OrderedDict\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import random\n",
    "import linecache\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import time\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25677175",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 4\n",
    "# it used to be 4, reduced for debugging\n",
    "BATCH_SIZE = 40\n",
    "SHUFFLE_BUFFER = 100\n",
    "PREFETCH_BUFFER = 10\n",
    "time_steps =48*3\n",
    "# *3\n",
    "interval = 20000 # DO NOT CHANGE!!!!! it was 1000 - this is roughly 80% of 25727 - worked with 20,000\n",
    "future_steps = 12\n",
    "split = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12a8a015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dataset):\n",
    "    def batch_format_fn(x_d, y_d):\n",
    "        return OrderedDict(x=x_d, y=y_d)\n",
    "\n",
    "    return dataset.repeat(NUM_EPOCHS).shuffle(SHUFFLE_BUFFER, seed=1).batch(\n",
    "      BATCH_SIZE).map(batch_format_fn).prefetch(PREFETCH_BUFFER)\n",
    "    \n",
    "def create_dataset_fed(files, lower, upper, time_steps=1):\n",
    "    xs, ys = [], []\n",
    "    for file in files:\n",
    "        x_t, y_t = [], []\n",
    "        data = file[lower:upper]\n",
    "        if data:\n",
    "            for i in range(len(data) - time_steps - 1 - future_steps):\n",
    "                v = data[i:(i + time_steps)] \n",
    "                z = data[(i + time_steps):(i + time_steps + future_steps)]\n",
    "                if check_nulls(z) and check_nulls(v):\n",
    "                    x_t.append(v)\n",
    "                    y_t.append(z)\n",
    "            x_t = np.array(x_t)[:,:,np.newaxis]\n",
    "            y_t = np.array(y_t)[:,:,np.newaxis]\n",
    "            xs.append(x_t)\n",
    "            ys.append(y_t)\n",
    "    xs = np.array(xs)\n",
    "    ys = np.array(ys)\n",
    "    # print(\"randomrandomrandom\")\n",
    "    # print(xs, \"\\n this is ys:\\n\", ys)\n",
    "    # return [tf.data.Dataset.from_tensor_slices((Xs[x],  np.array(ys[x]))) for x in range(len(Xs))]\n",
    "    # return [ tf.data.Dataset.from_tensor_slices((Xs[x],  np.array(ys[x]))) for x in range(len(Xs))] - removed brackets, and np.array\n",
    "    return [tf.data.Dataset.from_tensor_slices((xs[x], ys[x])) for x in range(len(xs))]\n",
    "\n",
    "def make_federated_data(files, lower, upper):\n",
    "    data = create_dataset_fed(files, lower, upper,time_steps)\n",
    "    return [preprocess(x) for x in data if x]\n",
    "\n",
    "def create_keras_model():\n",
    "    return tf.keras.models.Sequential([\n",
    "      keras.layers.LSTM(64, input_shape=(time_steps, 1)),\n",
    "      keras.layers.Dense(12),\n",
    "    ])\n",
    "\n",
    "def model_fn():\n",
    "    # We _must_ create a new model here, and _not_ capture it from an external\n",
    "    # scope. TFF will call this within different graph contexts.\n",
    "    keras_model = create_keras_model()\n",
    "    return tff.learning.from_keras_model(\n",
    "      keras_model,\n",
    "      input_spec=preprocessed_example_dataset.element_spec,\n",
    "      loss=tf.keras.losses.MeanSquaredError(),\n",
    "      metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "def check_nulls(data):\n",
    "    if not(all(is_float(ele) for ele in data)):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "def is_float(element):\n",
    "    try:\n",
    "        float(element)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9561aafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clusters = [['1086.txt',\n",
    "#   '1466.txt',\n",
    "#   '2521.txt',\n",
    "#   '2706.txt',\n",
    "#   '3418.txt',\n",
    "#   '3766.txt',\n",
    "#   '3780.txt',\n",
    "#   '4342.txt',\n",
    "#   '4690.txt',\n",
    "#   '4927.txt',\n",
    "#   '5062.txt',\n",
    "#   '5445.txt',\n",
    "#   '5618.txt',\n",
    "#   '5720.txt',\n",
    "#   '5844.txt',\n",
    "#   '5932.txt',\n",
    "#   '7116.txt'],\n",
    "#  ['2974.txt'],\n",
    "#  ['1448.txt',\n",
    "#   '1632.txt',\n",
    "#   '1758.txt',\n",
    "#   '1985.txt',\n",
    "#   '3757.txt',\n",
    "#   '4968.txt',\n",
    "#   '5583.txt',\n",
    "#   '7136.txt'],\n",
    "#  ['3676.txt', '5751.txt', '6044.txt', '7353.txt']]\n",
    "# use min three (have at least one with multple properties)\n",
    "clusters = [['2822.txt', '3490.txt', '1278.txt', '1652.txt', '2614.txt', '3394.txt', '2471.txt', '2890.txt', '2460.txt', '3421.txt', '1039.txt', '3001.txt', '3375.txt', '3406.txt', '2907.txt', '1360.txt', '2119.txt', '3672.txt', '3869.txt', '2780.txt', '3847.txt', '3338.txt'],\n",
    "            ['3028.txt'], \n",
    "            ['3650.txt', '4063.txt', '3486.txt', '1086.txt', '1733.txt', '1901.txt', '2211.txt', '3182.txt', '1838.txt', '2885.txt', '3769.txt', '4009.txt', '2492.txt', '1170.txt', '2096.txt', '2733.txt', '2041.txt', '3822.txt', '2137.txt', '2494.txt', '1149.txt', '2118.txt', '1516.txt', '3843.txt', '3060.txt', '1716.txt', '2555.txt', '1706.txt', '3850.txt', '2623.txt', '1513.txt'], \n",
    "            ['4154.txt'], \n",
    "            ['1285.txt', '3669.txt', '3936.txt', '3885.txt'], \n",
    "            ['1568.txt']]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6cb4d7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'2822.txt': 0, '3490.txt': 0, '1278.txt': 0, '1652.txt': 0, '2614.txt': 0, '3394.txt': 0, '2471.txt': 0, '2890.txt': 0, '2460.txt': 0, '3421.txt': 0, '1039.txt': 0, '3001.txt': 0, '3375.txt': 0, '3406.txt': 0, '2907.txt': 0, '1360.txt': 0, '2119.txt': 0, '3672.txt': 0, '3869.txt': 0, '2780.txt': 0, '3847.txt': 0, '3338.txt': 0, '3028.txt': 1, '3650.txt': 2, '4063.txt': 2, '3486.txt': 2, '1086.txt': 2, '1733.txt': 2, '1901.txt': 2, '2211.txt': 2, '3182.txt': 2, '1838.txt': 2, '2885.txt': 2, '3769.txt': 2, '4009.txt': 2, '2492.txt': 2, '1170.txt': 2, '2096.txt': 2, '2733.txt': 2, '2041.txt': 2, '3822.txt': 2, '2137.txt': 2, '2494.txt': 2, '1149.txt': 2, '2118.txt': 2, '1516.txt': 2, '3843.txt': 2, '3060.txt': 2, '1716.txt': 2, '2555.txt': 2, '1706.txt': 2, '3850.txt': 2, '2623.txt': 2, '1513.txt': 2, '4154.txt': 3, '1285.txt': 4, '3669.txt': 4, '3936.txt': 4, '3885.txt': 4, '1568.txt': 5}\n"
     ]
    }
   ],
   "source": [
    "# Read files in a dictionary with key = file name, and value = file\n",
    "\n",
    "\n",
    "C1086 = len(clusters)\n",
    "PATH = './ExperementData/'\n",
    "\n",
    "alloc = {}\n",
    "for c in range(len(clusters)):\n",
    "    for name in clusters[c]:\n",
    "        alloc[name] = c\n",
    "print(alloc)\n",
    "        \n",
    "files_in_cluster = []\n",
    "for c in range(len(clusters) + 1):\n",
    "    files_in_cluster.append([])\n",
    "# print(files_in_cluster)\n",
    "\n",
    "data = []\n",
    "\n",
    "onlyfiles = [f for f in listdir(PATH) if isfile(join(PATH, f))and f[-4:]==\".txt\"]\n",
    "for f in onlyfiles:\n",
    "    with open (PATH + f,'r') as reader:\n",
    "        temp = []\n",
    "        for line in reader:\n",
    "            if line.strip() == 'Null':\n",
    "                temp.append('Null')\n",
    "            else:\n",
    "                temp.append(float(line.strip()))\n",
    "                # temp.append(line.strip())\n",
    "        # print(f, alloc[f])\n",
    "        data.append(temp)\n",
    "        files_in_cluster[alloc[f]].append(temp)\n",
    "        if f == \"1086.txt\":\n",
    "            files_in_cluster[C1086].append(temp)\n",
    "\n",
    "test = [d[int(25727*split):] for d in data]\n",
    "\n",
    "# print(files_in_cluster)\n",
    "# print(files_in_cluster[1][1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "04b68dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomDict = make_federated_data(files_in_cluster[1], 1, 1 + interval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7eb59d22",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/by/4rt6rrtn24n6r36ktnkzt6q00000gn/T/ipykernel_30489/3823648567.py:24: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  xs = np.array(xs)\n",
      "/var/folders/by/4rt6rrtn24n6r36ktnkzt6q00000gn/T/ipykernel_30489/3823648567.py:25: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  ys = np.array(ys)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round  0, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('root_mean_squared_error', 0.6492432), ('loss', 0.42151675), ('num_examples', 1722412), ('num_batches', 43070)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
      "260\n",
      "round  0, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('root_mean_squared_error', 0.63243085), ('loss', 0.39996877), ('num_examples', 1721916), ('num_batches', 43057)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
      "242\n",
      "round  0, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('root_mean_squared_error', 0.6238179), ('loss', 0.3891486), ('num_examples', 1721844), ('num_batches', 43054)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
      "458\n",
      "round  1, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('root_mean_squared_error', 0.91989684), ('loss', 0.8462103), ('num_examples', 78744), ('num_batches', 1969)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
      "415\n",
      "round  1, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('root_mean_squared_error', 0.83646595), ('loss', 0.69967556), ('num_examples', 78744), ('num_batches', 1969)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
      "478\n",
      "round  2, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('root_mean_squared_error', 0.4311265), ('loss', 0.18587007), ('num_examples', 2278860), ('num_batches', 56987)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
      "297\n",
      "round  2, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('root_mean_squared_error', 0.4246601), ('loss', 0.18033622), ('num_examples', 2281032), ('num_batches', 57041)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
      "488\n",
      "round  2, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('root_mean_squared_error', 0.41764447), ('loss', 0.17442688), ('num_examples', 2278740), ('num_batches', 56984)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
      "444\n",
      "round  3, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('root_mean_squared_error', 4.0722795), ('loss', 16.583473), ('num_examples', 26212), ('num_batches', 656)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
      "500\n",
      "round  3, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('root_mean_squared_error', 2.7000725), ('loss', 7.2903905), ('num_examples', 25988), ('num_batches', 650)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
      "419\n",
      "round  3, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('root_mean_squared_error', 2.4544606), ('loss', 6.0243793), ('num_examples', 26312), ('num_batches', 658)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
      "245\n",
      "round  3, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('root_mean_squared_error', 2.2634428), ('loss', 5.123177), ('num_examples', 27008), ('num_batches', 676)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
      "238\n",
      "round  4, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('root_mean_squared_error', 1.0376804), ('loss', 1.0767807), ('num_examples', 261732), ('num_batches', 6545)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
      "241\n",
      "round  4, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('root_mean_squared_error', 0.9637183), ('loss', 0.928753), ('num_examples', 261720), ('num_batches', 6545)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
      "534\n",
      "round  4, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('root_mean_squared_error', 0.9455423), ('loss', 0.89405036), ('num_examples', 260548), ('num_batches', 6515)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
      "452\n",
      "round  5, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('root_mean_squared_error', 2.3941069), ('loss', 5.7317443), ('num_examples', 58576), ('num_batches', 1465)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
      "30\n",
      "round  5, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('root_mean_squared_error', 2.258338), ('loss', 5.100089), ('num_examples', 60264), ('num_batches', 1507)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n"
     ]
    }
   ],
   "source": [
    "# Understand interval, time_steps, future_steps, and the NULL values. Then the 20-80 and if there is overlapping train and test data.\n",
    "# 1086.txt\n",
    "example_dataset = create_dataset_fed(files_in_cluster[C1086], 1, 500, time_steps)[0]\n",
    "preprocessed_example_dataset = preprocess(example_dataset)\n",
    "# process = [tff.learning.build_federated_averaging_process - old code, had to update it\n",
    "process = [tff.learning.algorithms.build_weighted_fed_avg(\n",
    "    model_fn,\n",
    "    client_optimizer_fn=lambda: keras.optimizers.Adam(0.001),\n",
    "    server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1.0)) for _ in range(len(clusters))]\n",
    "          \n",
    "state = [process[x].initialize() for x in range (len(process))]\n",
    "for c in range(len(clusters)):\n",
    "    max_time = (len(clusters[c])/60)*9000 # CHANGE THIS TO 9000 and 30\n",
    "    start_time = time.time()\n",
    "    while time.time() - start_time < max_time:\n",
    "        location = random.randint(1,int(25727*split)-interval)\n",
    "        print(location)\n",
    "        federated_train_data = make_federated_data(files_in_cluster[c], location, location + interval)\n",
    "        if len(federated_train_data) > 0:\n",
    "#             state[c], metrics = process[c].next(state[c], federated_train_data) - old code, does not iterate\n",
    "#             is the below the right thing for state? It does not work for state[c] as this is not iterable\n",
    "            result = process[c].next(state[c], federated_train_data)\n",
    "            state[c] = result.state\n",
    "            metrics = result.metrics\n",
    "            print('round {:2d}, metrics={}'.format(c, metrics))\n",
    "#     tensorflow 2.9.0 and federated 0.40.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1ac4e51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data, time_steps=1):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(data) - time_steps-12):\n",
    "        v = data[i:(i + time_steps)]\n",
    "        z = data[(i + time_steps):(i + time_steps+12)]\n",
    "        if check_nulls(z) and check_nulls(v):\n",
    "            ys.append(z)\n",
    "            Xs.append(v)\n",
    "    return np.array(Xs), np.array(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "47706720",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156/156 [==============================] - 4s 20ms/step\n",
      "150/150 [==============================] - 4s 20ms/step\n",
      "156/156 [==============================] - 4s 21ms/step\n",
      "156/156 [==============================] - 4s 20ms/step\n",
      "156/156 [==============================] - 3s 17ms/step\n",
      "156/156 [==============================] - 4s 19ms/step\n",
      "156/156 [==============================] - 3s 18ms/step\n",
      "156/156 [==============================] - 3s 19ms/step\n",
      "156/156 [==============================] - 4s 18ms/step\n",
      "156/156 [==============================] - 4s 20ms/step\n",
      "156/156 [==============================] - 4s 20ms/step\n",
      "44/44 [==============================] - 1s 20ms/step\n",
      "156/156 [==============================] - 4s 20ms/step\n",
      "156/156 [==============================] - 3s 18ms/step\n",
      "150/150 [==============================] - 3s 19ms/step\n",
      "149/149 [==============================] - 4s 21ms/step\n",
      "156/156 [==============================] - 3s 18ms/step\n",
      "156/156 [==============================] - 4s 19ms/step\n",
      "156/156 [==============================] - 3s 19ms/step\n",
      "149/149 [==============================] - 4s 20ms/step\n",
      "156/156 [==============================] - 4s 19ms/step\n",
      "156/156 [==============================] - 3s 19ms/step\n",
      "156/156 [==============================] - 3s 18ms/step\n",
      "156/156 [==============================] - 3s 19ms/step\n",
      "156/156 [==============================] - 3s 19ms/step\n",
      "156/156 [==============================] - 3s 19ms/step\n",
      "156/156 [==============================] - 4s 19ms/step\n",
      "156/156 [==============================] - 3s 19ms/step\n",
      "156/156 [==============================] - 3s 19ms/step\n",
      "156/156 [==============================] - 3s 19ms/step\n",
      "156/156 [==============================] - 4s 19ms/step\n",
      "150/150 [==============================] - 3s 19ms/step\n",
      "156/156 [==============================] - 4s 20ms/step\n",
      "150/150 [==============================] - 4s 20ms/step\n",
      "156/156 [==============================] - 3s 18ms/step\n",
      "156/156 [==============================] - 3s 19ms/step\n",
      "150/150 [==============================] - 3s 19ms/step\n",
      "156/156 [==============================] - 3s 19ms/step\n",
      "156/156 [==============================] - 3s 18ms/step\n",
      "65/65 [==============================] - 2s 19ms/step\n",
      "156/156 [==============================] - 4s 19ms/step\n",
      "156/156 [==============================] - 3s 18ms/step\n",
      "156/156 [==============================] - 3s 19ms/step\n",
      "156/156 [==============================] - 4s 20ms/step\n",
      "156/156 [==============================] - 4s 19ms/step\n",
      "156/156 [==============================] - 3s 19ms/step\n",
      "156/156 [==============================] - 3s 19ms/step\n",
      "156/156 [==============================] - 4s 20ms/step\n",
      "156/156 [==============================] - 3s 19ms/step\n",
      "156/156 [==============================] - 3s 19ms/step\n",
      "149/149 [==============================] - 4s 19ms/step\n",
      "156/156 [==============================] - 3s 19ms/step\n",
      "156/156 [==============================] - 3s 18ms/step\n"
     ]
    }
   ],
   "source": [
    "# NEED TO UNDERSTAND WHY THERE ARE NULLS\n",
    "# fix 80/20 thing?\n",
    "# why is ther 12 columns in the files\n",
    "# WHY DO I HAVE A RESULT FOR 60 FILES AND NOT 12 - we train on all files but 80% of the readings and we test on all the houses in experment data but we test on the rest of the 20% of the readings\n",
    "# why is the training so weird? - trains per cluster and updates the weights in the alloted time so three times per cluster \n",
    "\n",
    "# print(len(test))\n",
    "for i in range(len(test)):\n",
    "    X_test, y_test = create_dataset(test[i], time_steps)\n",
    "#     print(\"HEREEeeeeeeeeeee\")\n",
    "#     print(len(X_test),len(y_test) )\n",
    "#     print(X_test, \"ihbserflijbenlihfb\\n\", y_test)\n",
    "    if X_test.size > 0 and y_test.size > 0:\n",
    "        X_test = X_test[:,:,np.newaxis]\n",
    "        y_test = y_test[:,:,np.newaxis]\n",
    "        model_for_inference = create_keras_model()\n",
    "        weights = state[alloc[onlyfiles[i]]].global_model_weights\n",
    "    #     weights = process[c].get_model_weights(state[alloc[onlyfiles[i]]]) = guesses\n",
    "        weights.assign_weights_to(model_for_inference)\n",
    "    #     state[alloc[onlyfiles[i]]].model.assign_weights_to(model_for_inference) = old code\n",
    "\n",
    "    #     process.get_model_weights(state[alloc[onlyfiles[i]]]) = what i found online\n",
    "\n",
    "    #     assigning weights from fed model to new keras model for prediciton\n",
    "        y_pred = model_for_inference.predict(X_test)\n",
    "        dataframe = pd.DataFrame(np.squeeze(np.array(y_pred)))\n",
    "    #     dataframe.to_csv(r\"./fed_weight_clust/pred/\"+onlyfiles[i][:4]+'.csv')\n",
    "        dataframe.to_csv(r\"./num_clust_six/pred-kmeans/\"+onlyfiles[i][:4]+'.csv')\n",
    "        dataframe = pd.DataFrame(np.squeeze(np.array(y_test)))\n",
    "    #     dataframe.to_csv(r\"./fed_weight_clust/test/\"+onlyfiles[i][:4]+'.csv')\n",
    "        dataframe.to_csv(r\"./num_clust_six/test-kmeans/\"+onlyfiles[i][:4]+'.csv')\n",
    "    \n",
    "    \n",
    "    \n",
    "#     created new keras model then assiging weights from other model to this one\n",
    "#     find new method for fed learning and making new predicitions (.predict or smth)\n",
    "# .predict\n",
    "# state is state of fed model that has been trained (model for prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e69bbfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
